{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854967b0",
   "metadata": {},
   "source": [
    "**Character-Level Text Generation with Vanilla RNN** \n",
    "\n",
    "**Goal**: Train an RNN to generate text character by character.\n",
    "\n",
    "**Tasks**\n",
    "Implement a simple RNN from scratch using NumPy (or PyTorch/TensorFlow for automatic differentiation).\\\n",
    "Train on a small text corpus (e.g., Shakespeare, Wikipedia snippets, or song lyrics).\\\n",
    "Experiment with:\\\n",
    "\t- Different hidden layer sizes\\\n",
    "\t- Sequence lengths (how many characters to unroll)\\\n",
    "\t- Temperature sampling (controlling randomness in generation).\\\n",
    "Compare outputs between trained and untrained models.\n",
    "  \n",
    "**Expected Outcome**:  \n",
    "The model should generate somewhat coherent (but imperfect) text.\n",
    "Understand vanishing gradients and why simple RNNs struggle with long sequences.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8524a",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc56b1",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b44b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "model_VanillaRNN = Sequential()\n",
    "model_VanillaRNN.add(tf.keras.layers.Input(shape=(None, 1))) # Assuming every character is a single feature and also time t\n",
    "model_VanillaRNN.add(tf.keras.layers.Dense(64, activation='tanh'))\n",
    "\n",
    "# will have to bulid a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f675d50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[0.99990916, 0.99990916, 0.99990916, 0.99990916, 0.99990916]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.ones((1,5,))\n",
    "dense = tf.keras.layers.Dense(5, activation='tanh')\n",
    "dense.build((None, 5))\n",
    "dense.set_weights([tf.ones((5, 5)), tf.zeros((5,))])\n",
    "dense(data)\n",
    "\n",
    "# seems like a good way apply layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f12ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self):\n",
    "        self.input = tf.ones((10,10,))\n",
    "\n",
    "        self.layer1 = tf.keras.layers.Dense(10, activation='tanh')\n",
    "        self.layer1.build((None, 10))\n",
    "        print(self.layer1(self.input))\n",
    "# made it a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7614db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]\n",
      " [ 0.48476276 -0.356358   -0.36308432 -0.9673964   0.9025267  -0.74758697\n",
      "  -0.47762728  0.3305781  -0.9375403   0.97452015]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "v = VanillaRNN()\n",
    "v.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4545317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self):\n",
    "        self.hidden_state1 = None\n",
    "\n",
    "    def fit(self, text=[0,1,1,0,1,0,1,1,0,1]):\n",
    "        self.input = tf.ones((10,10,))\n",
    "\n",
    "        self.layer1 = Dense(10, activation='linear')\n",
    "        self.layer1.build((None, 10))\n",
    "\n",
    "        for t in text:\n",
    "            self.hidden_state1 = tf.tanh(self.layer1() + self.layer1(self.hidden_state1))\n",
    "\n",
    "        return self.layer1(self.hidden_state1)\n",
    "# Need to research the structure of hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985e9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self):\n",
    "        self.A = 10\n",
    "        self.B = 10\n",
    "        self.time = 20\n",
    "\n",
    "        self.hidden_state1 = np.random.random((1, self.A))  # 1xA\n",
    "        self.layer1 = Dense(self.B, activation='linear')  # B\n",
    "\n",
    "    def fit(self, text=None):\n",
    "        # Test data\n",
    "        text = np.random.random((self.time, 1, self.A))  # Tx1xA\n",
    "\n",
    "        for Ti in range(self.time):\n",
    "            s1 = self.layer1(text[Ti])  # 1xA x AxB -> 1xB\n",
    "            s2 = self.layer1(self.hidden_state1)  #  1xA x AxB -> 1xB\n",
    "            \n",
    "            self.hidden_state1 = tf.tanh(s1 + s2)  # 1xB\n",
    "\n",
    "        return self.layer1(self.hidden_state1)  # 1xB\n",
    "    \n",
    "    # Need to update weights and fix update for hidden state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190c69e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.95217055  0.18014412  0.09028164  0.18123281 -0.01465768 -0.5539251\n",
      "  -0.6086999  -1.163328   -0.21046673  0.97643435]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = VanillaRNN()\n",
    "output = model.fit()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19b6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self):\n",
    "        self.A = 10\n",
    "        self.B = 10\n",
    "        self.time = 20\n",
    "\n",
    "        self.hidden_state1 = np.random.random((1, self.A))  # 1xA\n",
    "        self.layer1 = Dense(self.B, activation='linear')  # all x B\n",
    "\n",
    "    def fit(self, text=None):\n",
    "        # Test data\n",
    "        text = np.random.random((self.time, 1, self.A))  # Tx1xA\n",
    "\n",
    "        for Ti in range(self.time):\n",
    "            s1 = self.layer1(text[Ti])  # 1xA x AxB -> 1xB\n",
    "            s2 = self.layer1(self.hidden_state1)  #  1xA x AxB -> 1xB\n",
    "            \n",
    "            self.hidden_state1 = tf.tanh(s1 + s2)  # 1xB\n",
    "\n",
    "        return self.layer1(self.hidden_state1)  # 1xB\n",
    "    \n",
    "    # Need to update weights and fix update for hidden state\n",
    "    # Moreover this whole model is a single layer and doesn't seem extendable\n",
    "    # So it would seem that we would need a different approach to build a RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef647f",
   "metadata": {},
   "source": [
    "**Review**\n",
    "\n",
    "The above code looks something like a hidden state update. This model does not fullfil **REQ-003: CONFIGUREABLE HIDDEN LAYER SIZES** among others.\n",
    "\n",
    "**Possible models for mutiple layers:**\n",
    "- List of layer-objects\n",
    "- List of layer-matrices /  Tensor of layer-matrices\n",
    "- Using some tensorflow specific architecture for multiple layers or networks\n",
    "\n",
    "\n",
    "**Possible solutions:**\n",
    "- Take a look at functional keras API \n",
    "-  Subclass the model class\n",
    "    - Use custom call and init \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb81f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class HiddenState(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(HiddenState, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.hidden_state = self.add_weight(\n",
    "            shape=(1, units),\n",
    "            initializer=\"random_normal\" # or zeros etc\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cb2510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1)\n",
      "(25, 10)\n",
      "(25, 5)\n"
     ]
    }
   ],
   "source": [
    "A = 10\n",
    "B = 5\n",
    "\n",
    "batch_size = 25\n",
    "input_layer = tf.random.normal((batch_size, 1)) # Ax1\n",
    "\n",
    "hidden_state1 = HiddenState(units=A)  # 1xA\n",
    "\n",
    "layer1 = Dense(units=B, activation='linear')  # B x all\n",
    "layer1.build((None, A))\n",
    "\n",
    "print(input_layer.shape)\n",
    "x = hidden_state1(input_layer)\n",
    "print(x.shape) \n",
    "x = layer1(x) # 25xA *xB -> 25xB\n",
    "print(x.shape) \n",
    "# output : 25xB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b68cecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25, 5), dtype=float32, numpy=\n",
       "array([[ 0.009206  ,  0.02962599,  0.00361145, -0.00293425,  0.00607665],\n",
       "       [ 0.01516538,  0.04880397,  0.00594927, -0.0048337 ,  0.01001029],\n",
       "       [-0.00819033, -0.02635743, -0.00321301,  0.00261052, -0.00540623],\n",
       "       [ 0.01466852,  0.04720503,  0.00575436, -0.00467534,  0.00968232],\n",
       "       [-0.0245201 , -0.07890856, -0.00961907,  0.00781536, -0.0161851 ],\n",
       "       [-0.00754231, -0.02427203, -0.0029588 ,  0.00240398, -0.00497849],\n",
       "       [-0.00832139, -0.02677922, -0.00326442,  0.0026523 , -0.00549274],\n",
       "       [ 0.00589585,  0.01897356,  0.0023129 , -0.0018792 ,  0.00389171],\n",
       "       [-0.02860088, -0.09204098, -0.01121993,  0.00911603, -0.01887872],\n",
       "       [-0.01550188, -0.04988687, -0.00608128,  0.00494095, -0.0102324 ],\n",
       "       [-0.02481974, -0.07987285, -0.00973662,  0.00791086, -0.01638289],\n",
       "       [-0.00486389, -0.01565257, -0.00190807,  0.00155028, -0.00321053],\n",
       "       [ 0.01525478,  0.04909168,  0.00598434, -0.0048622 ,  0.0100693 ],\n",
       "       [-0.00559945, -0.01801969, -0.00219663,  0.00178473, -0.00369606],\n",
       "       [-0.01596106, -0.05136458, -0.00626141,  0.00508731, -0.0105355 ],\n",
       "       [-0.01796223, -0.05780457, -0.00704646,  0.00572515, -0.01185642],\n",
       "       [-0.03823616, -0.12304847, -0.01499979,  0.01218711, -0.02523874],\n",
       "       [-0.00182844, -0.00588415, -0.00071729,  0.00058278, -0.00120691],\n",
       "       [-0.00477777, -0.01537543, -0.00187429,  0.00152283, -0.00315369],\n",
       "       [ 0.01939681,  0.06242122,  0.00760924, -0.0061824 ,  0.01280335],\n",
       "       [ 0.01787741,  0.05753162,  0.00701319, -0.00569812,  0.01180043],\n",
       "       [ 0.05216068,  0.16785923,  0.02046228, -0.01662531,  0.03442996],\n",
       "       [ 0.02864595,  0.09218603,  0.01123761, -0.0091304 ,  0.01890847],\n",
       "       [ 0.02669174,  0.08589716,  0.01047099, -0.00850753,  0.01761854],\n",
       "       [-0.0137744 , -0.04432764, -0.0054036 ,  0.00439035, -0.00909214]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc06a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Variable path=hidden_state/variable, shape=(1, 10), dtype=float32, value=[[ 0.04654356 -0.02881666 -0.02108963 -0.00619787 -0.05357994  0.02723821\n",
       "   0.00670306 -0.04887664 -0.00453065 -0.01233678]]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state1.hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a6277",
   "metadata": {},
   "source": [
    "Now we need to define the model and its call and then compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09de44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class HiddenState(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(HiddenState, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.hidden_state = self.add_weight(\n",
    "            shape=(1, units),\n",
    "            initializer=\"random_normal\" # or zeros etc\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.hidden_state)\n",
    "    \n",
    "\n",
    "class VanillaRNN(tf.keras.Model):\n",
    "    def __init__(self, n_hidden_layers : int, units_per_layer : list):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.layers = []\n",
    "        for _, units in enumerate(units_per_layer):\n",
    "            self.layers.append(Dense(units, activation='tanh'))\n",
    "\n",
    "        self.hidden_state = HiddenState(units=units_per_layer[0])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f6f27",
   "metadata": {},
   "source": [
    "**Review**\n",
    "- Apparently every layer must have its own hidden state. That mean we have two options:\n",
    "    - Make a rnn-layer after incl hidden state based on Layer or Dense\n",
    "    - Make a list of hidden states for the layers and incl it in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6233513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class HiddenState(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(HiddenState, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.hidden_state = self.add_weight(\n",
    "            shape=(1, units),\n",
    "            initializer=\"random_normal\" # or zeros etc\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.hidden_state)\n",
    "    \n",
    "\n",
    "class VanillaRNN(tf.keras.Model):\n",
    "    def __init__(self, n_hidden_layers : int, units_per_layer : list):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "        self.hidden_states = []\n",
    "\n",
    "        for _, units in enumerate(units_per_layer):\n",
    "            self.layers.append(Dense(units, activation='tanh'))\n",
    "            self.hidden_states.append(HiddenState(units=units))\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Procedure in psuedo code:\n",
    "        at time step t:\n",
    "            update hidden states as you forward propogate\n",
    "            \n",
    "            out of model:\n",
    "                compile with adam, and sparse crossentropy loss\n",
    "                optimize with fit\n",
    "\n",
    "\n",
    "        Update hidden state:\n",
    "            h =  tanh(Wx * x + Wh * h + b)\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b4070",
   "metadata": {},
   "source": [
    "Apparently there needs to be two weights in each layer:\n",
    "- W_h for hidden state\n",
    "- W_x for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class LayerRNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(LayerRNN, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.W_x = None\n",
    "        self.W_h = None\n",
    "\n",
    "        self.hidden_state = None\n",
    "        \n",
    "        self.b = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        batch_size = input_shape[0]\n",
    "        input_shape = input_shape[-1]\n",
    "\n",
    "\n",
    "        self.W_x = self.add_weight(\n",
    "            shape=(input_shape, self.units),\n",
    "            initializer=\"random_normal\", # or zeros etc\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.W_h = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer=\"random_normal\", # or zeros etc\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.hidden_state = tf.zeros((batch_size, self.units))\n",
    "\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs, reset_state=False):\n",
    "        if reset_state:\n",
    "            self.hidden_state = tf.zeros_like(self.hidden_state)\n",
    "\n",
    "        x = tf.matmul(inputs, self.W_x)  \n",
    "        h = tf.matmul(self.hidden_state, self.W_h)\n",
    "        self.hidden_state = tf.tanh(x + h + self.b)\n",
    "\n",
    "        return self.hidden_state\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class VanillaRNN(tf.keras.Model):\n",
    "    def __init__(self, n_hidden_layers : int, units_per_layer : list):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "\n",
    "        self.layer_list = []\n",
    "\n",
    "        for _, units in enumerate(units_per_layer):\n",
    "            self.layer_list.append(LayerRNN(units, activation='tanh'))\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Procedure in psuedo code:\n",
    "        at time step t:\n",
    "            update hidden states as you forward propogate\n",
    "            \n",
    "            out of model:\n",
    "                compile with adam, and sparse crossentropy loss\n",
    "                optimize with fit\n",
    "\n",
    "\n",
    "        Update hidden state:\n",
    "            h =  tanh(Wx * x + Wh * h + b)\n",
    "        \"\"\"\n",
    "        batch_size = inputs.shape[0]\n",
    "        time_steps = inputs.shape[1]\n",
    "\n",
    "        for T in range(time_steps):\n",
    "            x  = inputs[:, T, :]\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229607e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avidu\\OneDrive\\Documents\\VanillaRNN-TextGenerator\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'vanilla_rnn', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vanilla_rnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vanilla_rnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn (\u001b[38;5;33mLayerRNN\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_1 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_2 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "VanillaRNNModel = VanillaRNN(n_hidden_layers=3, units_per_layer=[10, 20, 30])\n",
    "VanillaRNNModel.build((None, 5, 10))  # Assuming input shape is (batch_size, time_steps, features)\n",
    "print(VanillaRNNModel.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd3913ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling VanillaRNN.call().\n\n\u001b[1mExpected int32, but got None of type 'NoneType'.\u001b[0m\n\nArguments received by VanillaRNN.call():\n  • inputs=tf.Tensor(shape=(None, 10, 5), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[32m     25\u001b[39m loss, accuracy = model.evaluate(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\avidu\\OneDrive\\Documents\\VanillaRNN-TextGenerator\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mVanillaRNN.call\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m     86\u001b[39m x  = inputs[:, T, :]\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mLayerRNN.build\u001b[39m\u001b[34m(self, input_shape)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.W_x = \u001b[38;5;28mself\u001b[39m.add_weight(\n\u001b[32m     24\u001b[39m     shape=(input_shape, \u001b[38;5;28mself\u001b[39m.units),\n\u001b[32m     25\u001b[39m     initializer=\u001b[33m\"\u001b[39m\u001b[33mrandom_normal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# or zeros etc\u001b[39;00m\n\u001b[32m     26\u001b[39m     trainable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.W_h = \u001b[38;5;28mself\u001b[39m.add_weight(\n\u001b[32m     30\u001b[39m     shape=(\u001b[38;5;28mself\u001b[39m.units, \u001b[38;5;28mself\u001b[39m.units),\n\u001b[32m     31\u001b[39m     initializer=\u001b[33m\"\u001b[39m\u001b[33mrandom_normal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# or zeros etc\u001b[39;00m\n\u001b[32m     32\u001b[39m     trainable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mself\u001b[39m.hidden_state = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.b = \u001b[38;5;28mself\u001b[39m.add_weight(\n\u001b[32m     38\u001b[39m     shape=(\u001b[38;5;28mself\u001b[39m.units,),\n\u001b[32m     39\u001b[39m     initializer=\u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     40\u001b[39m     trainable=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     41\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Exception encountered when calling VanillaRNN.call().\n\n\u001b[1mExpected int32, but got None of type 'NoneType'.\u001b[0m\n\nArguments received by VanillaRNN.call():\n  • inputs=tf.Tensor(shape=(None, 10, 5), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(num_samples, sequence_length, num_features):\n",
    "    X = np.random.rand(num_samples, sequence_length, num_features)  # Random sequences\n",
    "    y = np.sum(X, axis=(1, 2)) % 2  # Labels: 0 if sum is even, 1 if odd\n",
    "    return X, y\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1000\n",
    "sequence_length = 10\n",
    "num_features = 5\n",
    "n_hidden_layers = 2\n",
    "units_per_layer = [64, 32]\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_data(num_samples, sequence_length, num_features)\n",
    "\n",
    "# Create and compile the model\n",
    "model = VanillaRNN(n_hidden_layers=n_hidden_layers, units_per_layer=units_per_layer)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a0c4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class LayerRNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(LayerRNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.W_x = self.add_weight(\n",
    "            shape=(input_dim, self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.W_h = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, hidden_state):\n",
    "        x = tf.matmul(inputs, self.W_x)  \n",
    "        h = tf.matmul(hidden_state, self.W_h)\n",
    "        hidden_state = tf.tanh(x + h + self.b)\n",
    "\n",
    "        return hidden_state\n",
    "\n",
    "class VanillaRNN(tf.keras.Model):\n",
    "    def __init__(self, n_hidden_layers: int, units_per_layer: list):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.layers_list = [LayerRNN(units) for units in units_per_layer]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = inputs.shape[1]\n",
    "\n",
    "        # Initialize hidden state for the first layer\n",
    "        hidden_states = [tf.zeros((batch_size, layer.units)) for layer in self.layers_list]\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            x = inputs[:, t, :]  # Get the input for the current time step\n",
    "            for i, layer in enumerate(self.layers_list):\n",
    "                hidden_states[i] = layer(x, hidden_states[i])  # Update hidden state\n",
    "\n",
    "                # Pass the output to the next layer\n",
    "                x = hidden_states[i]\n",
    "\n",
    "        return hidden_states[-1]  # Return the output of the last layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3533c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avidu\\OneDrive\\Documents\\VanillaRNN-TextGenerator\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'vanilla_rnn_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vanilla_rnn_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vanilla_rnn_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn_5 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_6 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_7 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "VanillaRNNModel = VanillaRNN(n_hidden_layers=3, units_per_layer=[10, 20, 30])\n",
    "VanillaRNNModel.build((None, 5, 10))  # Assuming input shape is (batch_size, time_steps, features)\n",
    "print(VanillaRNNModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c060fed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vanilla_rnn_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vanilla_rnn_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn_8 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_9 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 5.7938\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.7056\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6942\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6929\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6944\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6925\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6923\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6914\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6906\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6900\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.6887 \n",
      "Loss: 0.6910579800605774, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "def generate_data(num_samples, sequence_length, num_features):\n",
    "    X = np.random.rand(num_samples, sequence_length, num_features)  # Random sequences\n",
    "    y = np.sum(X, axis=(1, 2)) % 2  # Labels: 0 if sum is even, 1 if odd\n",
    "    return X, y\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1000\n",
    "sequence_length = 10\n",
    "num_features = 5\n",
    "n_hidden_layers = 2\n",
    "units_per_layer = [64, 32]\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_data(num_samples, sequence_length, num_features)\n",
    "\n",
    "# Create and compile the model\n",
    "model = VanillaRNN(n_hidden_layers=n_hidden_layers, units_per_layer=units_per_layer)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ccd4355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vanilla_rnn_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vanilla_rnn_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerRNN</span>)          │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ layer_rnn_8 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │         \u001b[38;5;34m4,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_rnn_9 (\u001b[38;5;33mLayerRNN\u001b[0m)          │ ?                      │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,754</span> (88.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m22,754\u001b[0m (88.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,584</span> (29.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,584\u001b[0m (29.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,170</span> (59.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m15,170\u001b[0m (59.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4016d",
   "metadata": {},
   "source": [
    "**Review**\n",
    "- First SUCCESSFUL run\n",
    "- What could y be/ what loss/ what metrics?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef242643",
   "metadata": {},
   "source": [
    "## ai_generated Review of Attempts and Progress\n",
    "\n",
    "### What You've Tried\n",
    "\n",
    "1. **Initial Exploration**\n",
    "    - Started with basic Keras/TensorFlow layers (Dense) to simulate RNN-like behavior.\n",
    "    - Experimented with custom classes and manual hidden state updates.\n",
    "\n",
    "2. **Custom RNN Layer Development**\n",
    "    - Created `HiddenState` and `LayerRNN` classes to encapsulate hidden state logic.\n",
    "    - Explored different ways to manage hidden states, including per-layer hidden states.\n",
    "    - Implemented custom weight matrices (`W_x`, `W_h`) and bias for each layer.\n",
    "\n",
    "3. **Model Architecture**\n",
    "    - Built a `VanillaRNN` model using lists of custom RNN layers.\n",
    "    - Iterated on the model’s `call` method to process sequences step-by-step and propagate hidden states through layers.\n",
    "\n",
    "4. **Data Generation and Training**\n",
    "    - Generated synthetic sequence data for testing.\n",
    "    - Compiled and trained the custom RNN model using Keras’ training loop.\n",
    "\n",
    "### What Worked\n",
    "\n",
    "- Successfully implemented a multi-layer vanilla RNN from scratch using TensorFlow’s low-level API.\n",
    "- Managed hidden states for each layer and updated them at each time step.\n",
    "- The model could be compiled, trained, and evaluated using standard Keras methods.\n",
    "- Achieved a working training loop with synthetic data and observed loss/accuracy metrics.\n",
    "\n",
    "### What Didn’t Work / Challenges\n",
    "\n",
    "- Early attempts using only Dense layers did not capture the recurrent nature of RNNs.\n",
    "- Managing hidden states outside of the Keras layer/model structure was cumbersome and error-prone.\n",
    "- Initial custom classes lacked proper weight management and did not generalize to multiple layers.\n",
    "- Some confusion around input/output shapes and how to propagate hidden states between layers and time steps.\n",
    "- Output layer and loss function needed to be carefully chosen to match the task (e.g., classification vs. regression).\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- Each RNN layer must maintain its own hidden state and update it at every time step.\n",
    "- Custom RNNs require careful management of weights and state, but TensorFlow’s subclassing API makes this possible.\n",
    "- Building from scratch deepens understanding of RNN internals, including vanishing gradients and sequence processing.\n",
    "- Keras’ model API allows integration of custom layers with standard training workflows.\n",
    "\n",
    "**Next Steps:**  \n",
    "- Experiment with different sequence lengths, hidden sizes, and temperature sampling for text generation.\n",
    "- Add output layers suitable for character-level prediction (e.g., softmax over vocabulary).\n",
    "- Try training on real text data and compare outputs from trained vs. untrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71918d4a",
   "metadata": {},
   "source": [
    "**Note** : The classes were moved to rnn.py and utils were implemented for vectorization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7586fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15s/step - accuracy: 0.0244 - loss: 3.0450\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1463 - loss: 3.0129\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.3171 - loss: 2.9809\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.4390 - loss: 2.9484\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.5610 - loss: 2.9149\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6341 - loss: 2.8797\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6829 - loss: 2.8423\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6829 - loss: 2.8020\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6829 - loss: 2.7580\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6829 - loss: 2.7096\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6098 - loss: 2.6560\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5366 - loss: 2.5970\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5122 - loss: 2.5340\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3659 - loss: 2.4721\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.2927 - loss: 2.4175\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3659 - loss: 2.3561\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4390 - loss: 2.2786\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5366 - loss: 2.1914\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6341 - loss: 2.0992\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6585 - loss: 2.0013\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6829 - loss: 1.8951\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7073 - loss: 1.7819\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7561 - loss: 1.7011\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7073 - loss: 1.5880\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7317 - loss: 1.5120\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7317 - loss: 1.4338\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7561 - loss: 1.3446\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7805 - loss: 1.2760\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8293 - loss: 1.2131\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8537 - loss: 1.1223\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9024 - loss: 1.0551\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9024 - loss: 0.9989\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9512 - loss: 0.9374\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9512 - loss: 0.8928\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9512 - loss: 0.8329\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9512 - loss: 0.7810\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9512 - loss: 0.7335\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9268 - loss: 0.6851\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9512 - loss: 0.6417\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9756 - loss: 0.6043\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9756 - loss: 0.5687\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9756 - loss: 0.5301\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9756 - loss: 0.4953\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.4646\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.4356\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.4077\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3815\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.3579\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.3360\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.3142\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.2940\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.2763\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.2605\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.2458\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.2318\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.2184\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.2061\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.1950\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.1845\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.1743\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1648\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1558\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.1475\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.1397\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.1326\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.1258\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1193\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.1133\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.1077\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1026\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0977\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0932\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0890\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0851\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0813\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0778\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0745\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0714\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0685\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0658\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0632\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0608\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0585\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0563\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0543\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0524\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0506\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0489\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0473\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0458\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0443\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0430\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0417\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0405\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0393\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0382\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0372\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0362\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0353\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1aaf22c3850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import Vectorizer\n",
    "from rnn import VanillaRNN\n",
    "\n",
    "\n",
    "# Sample text for training\n",
    "text = \"Pets are beloved companions that bring joy\"\n",
    "# Initialize the vectorizer\n",
    "vectorizer = Vectorizer(text)\n",
    "\n",
    "# Tokenize and vectorize the training text\n",
    "tokens = vectorizer.tokenize()\n",
    "vector_one_hot = vectorizer.vectorize(tokens)\n",
    "vocab_size = vectorizer.vocab_size\n",
    "\n",
    "# Initialize and compile the VanillaRNN model\n",
    "vanilla_rnn = VanillaRNN(n_hidden_layers=2, units_per_layer=[128, 64], vocab_size=vocab_size)\n",
    "vanilla_rnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare input and target for next-character prediction\n",
    "X = vector_one_hot[:, :-1, :]  # Input: all but last character\n",
    "y = np.argmax(vector_one_hot[:, 1:, :], axis=-1)  # Target: all but first character\n",
    "\n",
    "# Fit the model\n",
    "vanilla_rnn.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4557b8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming text:  are beloved companions that bring joyt bring joyt atibg nh ts that bring joyt bring joyt bring joyt bri\n",
      "Generated Text:\n",
      "are beloved companions that bring joyt bring joyt atibg nh ts that bring joyt bring joyt bring joyt bri\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, vectorizer, seed_text, num_chars, temperature=1.0):\n",
    "    # Tokenize and vectorize the seed text.\n",
    "    tokens = vectorizer.tokenize() if seed_text == vectorizer.text else [vectorizer.char2idx[c] for c in seed_text]\n",
    "    # Generate one-hot vectors; shape: (1, seq_len, vocab_size)\n",
    "    input_seq = vectorizer.vectorize(tokens)\n",
    "    generated = seed_text\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(num_chars):\n",
    "        # Predict next character probabilities from the current sequence.\n",
    "        logits = model(input_seq)  # shape: (1, time_steps, vocab_size)\n",
    "        last_logits = logits[0, -1, :] / temperature\n",
    "        probs = tf.nn.softmax(last_logits).numpy()\n",
    "\n",
    "             \n",
    "        # next_token = np.random.choice(range(vectorizer.vocab_size), p=probs)\n",
    "        # Not using distribution as that leads to non sensical results\n",
    "        \n",
    "        next_token = np.argmax(probs)\n",
    "        \n",
    "        # Append predicted character.\n",
    "        generated += vectorizer.idx2char[next_token]\n",
    "        # Append one-hot vector for the new token.\n",
    "        next_one_hot = tf.one_hot([next_token], depth=vectorizer.vocab_size)\n",
    "        next_one_hot = tf.expand_dims(next_one_hot, axis=1)  # shape: (1, 1, vocab_size)\n",
    "        input_seq = tf.concat([input_seq, next_one_hot], axis=1)\n",
    "\n",
    "        print(vectorizer.idx2char[next_token], end=\"\")\n",
    "    return generated\n",
    "\n",
    "\n",
    "\n",
    "seed = \"are\"\n",
    "\n",
    "print(\"Streaming text: \", seed, end=\"\")\n",
    "generated_text = generate_text(vanilla_rnn, vectorizer, seed, num_chars=100, temperature=2)\n",
    "print(\"\\nGenerated Text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
